Add to btCollisionObject:
    beryllEngineObjectID

Synchronize with mutex user collision callback:
    gContactAddedCallback = collisionsCallBack;

Update and synchronize btCollisionDispatcherMt.cpp

Delete custom threads handling and indexes:
    btGetCurrentThreadIndex()
    gThreadCounter.getNext()
    BT_MAX_THREAD_COUNT

Delete files:
    bullet/LinearMath/TaskScheduler/btThreadSupportInterface.h
    bullet/LinearMath/TaskScheduler/btThreadSupportPosix.cpp
    bullet/LinearMath/TaskScheduler/btThreadSupportWin32.cpp

    bullet/btBulletCollisionAll.cpp
    bullet/btBulletDynamicsAll.cpp
    bullet/btLinearMathAll.cpp

LinearMath/TaskScheduler/btTaskScheduler.cpp
    Replace to:

#if BT_THREADSAFE

#include "LinearMath/btThreads.h"

#include <vector>
#include <future>
#include <thread>
#include <algorithm>

class btTaskSchedulerForBeryll : public btITaskScheduler
{
public:
    btTaskSchedulerForBeryll()
    {
        // All available threads -1.
        m_numThreads = std::max((std::thread::hardware_concurrency() == 0 ? 0 : std::thread::hardware_concurrency() - 1u), 1u);

        m_futuresVoid.reserve(m_numThreads);
        m_futuresFloat.reserve(m_numThreads);
    }

    ~btTaskSchedulerForBeryll() override
    {
        m_futuresVoid.clear();
        m_futuresFloat.clear();
    }

    int getNumThreads() const override { return m_numThreads; }

    void parallelFor(int iBegin, int iEnd, int grainSize, const btIParallelForBody& body) override
    {
        int numberElements = iEnd - iBegin;

        if(numberElements < 4 || m_numThreads == 1)
        {
            // Run on main thread.
            body.forLoop(iBegin, iEnd);
        }
        else
        {
            m_futuresVoid.clear();

            int oneChunkSize = numberElements / m_numThreads;
            oneChunkSize++; // Make sure (oneChunkSize * m_numThreads) > numberElements.

            for(int i = iBegin; i < iEnd; i += oneChunkSize)
            {
                int chunkEnd = std::min(i + oneChunkSize, iEnd);

                m_futuresVoid.emplace_back(std::async(std::launch::async, &btIParallelForBody::forLoop, std::cref(body), i, chunkEnd));
            }

            // Wait for all threads.
            for(std::future<void>& ft : m_futuresVoid)
            {
                ft.wait();
            }
        }
    }

    float parallelSum(int iBegin, int iEnd, int grainSize, const btIParallelSumBody& body) override
    {
		float result = 0.0f;
        int numberElements = iEnd - iBegin;

        if(numberElements < 4 || m_numThreads == 1)
        {
            // Run on main thread.
            return body.sumLoop(iBegin, iEnd);
        }
        else
        {
            m_futuresFloat.clear();

            int oneChunkSize = numberElements / m_numThreads;
            oneChunkSize++; // Make sure (oneChunkSize * m_numThreads) > numberElements.

            for(int i = iBegin; i < iEnd; i += oneChunkSize)
            {
                int chunkEnd = std::min(i + oneChunkSize, iEnd);

                m_futuresFloat.emplace_back(std::async(std::launch::async, &btIParallelSumBody::sumLoop, std::cref(body), i, chunkEnd));
            }

            // Wait for all threads.
            for(std::future<float>& ft : m_futuresFloat)
            {
                result += ft.get();
            }
        }

        return result;
    }

private:
    int m_numThreads = 1;
    std::vector<std::future<void>> m_futuresVoid;
    std::vector<std::future<float>> m_futuresFloat;
};

btITaskScheduler* btCreateTaskSchedulerForBeryll()
{
    btTaskSchedulerForBeryll* ts = new btTaskSchedulerForBeryll();
    return ts;
}

#endif // #if BT_THREADSAFE
